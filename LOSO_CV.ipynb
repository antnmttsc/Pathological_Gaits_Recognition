{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HjYAs8hMfjG"
      },
      "source": [
        "# Pathological Gaits Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nphHJrVqfu6k"
      },
      "source": [
        "## 0 - General Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAco0tplP8l2",
        "outputId": "f8345d15-4b5e-4b4c-a276-2583760347e2"
      },
      "outputs": [],
      "source": [
        "!pip install -q keras==3.8.0 tensorflow==2.18.0\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "print(f\"Keras version: {keras.__version__}\")\n",
        "SEED_VALUE=821\n",
        "keras.utils.set_random_seed(SEED_VALUE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47CMH4ZVeo43"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import zipfile\n",
        "import sys\n",
        "import os\n",
        "import time\n",
        "import gc\n",
        "import pickle\n",
        "\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.utils import plot_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZ2tZd2SevaK"
      },
      "outputs": [],
      "source": [
        "with zipfile.ZipFile(\"Utils.zip\", \"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"Utils\")\n",
        "sys.path.append('/content/Utils')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoW5o0D8fHXA"
      },
      "outputs": [],
      "source": [
        "import Utils.Constants as _c\n",
        "\n",
        "from Utils.Preprocessing import create_reference_df, create_dataset, create_dataset_AE, create_fusion_dataset\n",
        "\n",
        "from Utils.LOSO_CV_Utils import (\n",
        "    print_params,\n",
        "    print_params_AE,\n",
        "    remove_cache_from_drive,\n",
        "    count_chunks,\n",
        "    plot_history,\n",
        "    print_time_info,\n",
        "    compute_metrics,\n",
        "    plot_history_AE,\n",
        "    print_time_info_fus,\n",
        "    plot_history_fus,\n",
        "    compute_metrics_fus,\n",
        "    plot_metrics_fus\n",
        ")\n",
        "\n",
        "from Utils.Analyze_Results import (\n",
        "    merge_results,\n",
        "    merge_loso_results,\n",
        "    print_overall_time,\n",
        "    print_overall_epochs,\n",
        "    print_overall_cm,\n",
        "    print_overall_clf_report,\n",
        "    print_overall_accuracies,\n",
        "    print_overall_history,\n",
        "    print_overall_history_AE,\n",
        "    print_overall_max_gpu_memory_usage,\n",
        "    show_multiple_accuracies\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LsWADzI3w4Um",
        "outputId": "edfaa9f4-8284-407a-ee3a-b9fdeca51b5c"
      },
      "outputs": [],
      "source": [
        "# connect to drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCunFZ9n1wOq"
      },
      "outputs": [],
      "source": [
        "data_dir = '' # data directory, e.g.: '/content/drive/MyDrive/Colab Notebooks/folder/data.zip'\n",
        "local_data_dir = '' # local data directory, e.g.: '/content/data'\n",
        "\n",
        "os.chdir('') # change to the directory where the notebook is located, e.g.: '/content/drive/MyDrive/Colab Notebooks/folder'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujnRfW8Qw77l"
      },
      "outputs": [],
      "source": [
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (10, 6)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "mpl.rcParams['legend.fontsize'] = 'large'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65lNf2T3KQGU"
      },
      "source": [
        "## 1 - Create Data Folder and Reference df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcCYaPNro8JX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!unzip \"{data_dir}\" -d \"{local_data_dir}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "MBBb3iLKl1dQ",
        "outputId": "b2207ab8-5fa0-41bd-d531-83a5acd10dd0"
      },
      "outputs": [],
      "source": [
        "reference_df = create_reference_df(local_data_dir, _c.convrt_gait_dict)\n",
        "\n",
        "test_reference_df = reference_df[reference_df['subject']=='subject12']\n",
        "loso_reference_df = reference_df[reference_df['subject']!='subject12']\n",
        "\n",
        "print(f'{loso_reference_df.shape=}')\n",
        "print(f'{test_reference_df.shape=}')\n",
        "loso_reference_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vzyNyxKcO0K"
      },
      "outputs": [],
      "source": [
        "del reference_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RC8sIB8vFEL_"
      },
      "source": [
        "## 2 - Models Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jgYaIOCFd_p"
      },
      "outputs": [],
      "source": [
        "def build_gru(\n",
        "  units:int,\n",
        "  return_sequences:bool,\n",
        "  bidirectional:bool,\n",
        "  seed_value:int=SEED_VALUE,\n",
        "  name:str=None\n",
        "):\n",
        "  gru = tf.keras.layers.GRU(units=units,\n",
        "                            return_sequences=return_sequences,\n",
        "                            kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value),\n",
        "                            recurrent_initializer=tf.keras.initializers.Orthogonal(seed=seed_value),\n",
        "                            bias_initializer=tf.keras.initializers.Zeros(),\n",
        "                            name=name)\n",
        "\n",
        "  return tf.keras.layers.Bidirectional(gru, name=name) if bidirectional else gru\n",
        "\n",
        "def build_dense(\n",
        "  units:int,\n",
        "  activation:str,\n",
        "  lambda_l2:float=None,\n",
        "  seed_value:int=SEED_VALUE,\n",
        "  name:str=None\n",
        "):\n",
        "  return tf.keras.layers.Dense(units=units, activation=activation,\n",
        "                               kernel_regularizer=tf.keras.regularizers.l2(lambda_l2),\n",
        "                               kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value),\n",
        "                               bias_initializer=tf.keras.initializers.Zeros(),\n",
        "                               name=name)\n",
        "\n",
        "def build_conv(\n",
        "  num_filters:int,\n",
        "  kernel_size:tuple,\n",
        "  activation:str,\n",
        "  padding:str,\n",
        "  seed_value:int=SEED_VALUE,\n",
        "  name:str=None\n",
        "):\n",
        "  return tf.keras.layers.Conv2D(num_filters, kernel_size, activation=activation, padding=padding,\n",
        "                                kernel_initializer=tf.keras.initializers.GlorotUniform(seed=seed_value),\n",
        "                                name=name)\n",
        "\n",
        "def build_AE_layer(\n",
        "  units:int,\n",
        "  name:str,\n",
        "  layer_type:str\n",
        "):\n",
        "  if layer_type == 'gru':\n",
        "    return tf.keras.layers.GRU(units=units, return_sequences=True, name=name)\n",
        "\n",
        "  else: # layer_type == 'lstm'\n",
        "    return tf.keras.layers.LSTM(units=units, return_sequences=True, name=name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Eokhp2KFeYJ"
      },
      "source": [
        "### 2.1 - Skeleton Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9eQ8-AhGVSV"
      },
      "source": [
        "#### 2.1.1 - GRU Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ys_fmAxNFsFW"
      },
      "outputs": [],
      "source": [
        "def GRU_Skeleton(input_shape:tuple, lambda_l2:float=None, bidirectional:bool=False):\n",
        "\n",
        "  X_input = tf.keras.Input(input_shape, name='sk_input')\n",
        "\n",
        "  # dense layer\n",
        "  X = build_dense(units=input_shape[1], activation='relu', name='dense_input_sk')(X_input)\n",
        "\n",
        "  # GRU layers\n",
        "  X = build_gru(256, return_sequences=True, bidirectional=bidirectional, name='gru_sk0')(X_input)\n",
        "  X = build_gru(256, return_sequences=True, bidirectional=bidirectional, name='gru_sk1')(X)\n",
        "  X = build_gru(128, return_sequences=True, bidirectional=bidirectional, name='gru_sk2')(X)\n",
        "  X = build_gru(128, return_sequences=False, bidirectional=bidirectional, name='gru_sk3')(X)\n",
        "\n",
        "  # dense and regularization layers\n",
        "  X = build_dense(units=100, lambda_l2=lambda_l2, activation=None, name='dense_sk0')(X)\n",
        "  X = tf.keras.layers.Dropout(0.5)(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=-1, name='batchnorm_sk')(X)\n",
        "\n",
        "  # dense layers with decreasing units\n",
        "  for idx, units in enumerate([128, 64, 32]):\n",
        "    X = build_dense(units=units, lambda_l2=lambda_l2, activation='relu', name=f'dense_sk{idx+1}')(X)\n",
        "\n",
        "  # output layer\n",
        "  X = build_dense(units=6, activation='softmax', name=f'dense_sk{idx+2}')(X)\n",
        "\n",
        "  model = tf.keras.Model(inputs=X_input, outputs=X, name='SkeletonModel')\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "1FNk4Wjq-Gg5",
        "outputId": "10d50392-f7bd-4e61-bd0f-a42c932918a2"
      },
      "outputs": [],
      "source": [
        "# num_joints = 3*len(_c.joints_to_include)\n",
        "num_joints = 3*len([0,1,2,3,4,11,18,19,20,21,22,23,24,25])\n",
        "target_size = 50\n",
        "input_shape = (target_size, num_joints)\n",
        "\n",
        "sk_gru_exemple = GRU_Skeleton(input_shape)\n",
        "\n",
        "sk_gru_exemple.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD1IvmPBGZ58"
      },
      "source": [
        "#### 2.1.2 - Transformer Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t52GHlyHMs8R"
      },
      "outputs": [],
      "source": [
        "# Positional Encoding Layer\n",
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "  def __init__(self, max_len=100):\n",
        "    super().__init__()\n",
        "    self.max_len = max_len\n",
        "\n",
        "  def build(self, input_shape):\n",
        "    self.pos_embedding = self.add_weight(\n",
        "      name=\"pos_embedding\",\n",
        "      shape=(self.max_len, input_shape[-1]),\n",
        "      initializer=\"random_normal\",\n",
        "      trainable=True,\n",
        "    )\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    return x + self.pos_embedding[:length]\n",
        "\n",
        "# Transformer Encoder Block\n",
        "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0.1):\n",
        "\n",
        "  attention_output = tf.keras.layers.MultiHeadAttention(\n",
        "    key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
        "  )(inputs, inputs)\n",
        "\n",
        "  attention_output = tf.keras.layers.Dropout(dropout)(attention_output)\n",
        "\n",
        "  X = tf.keras.layers.Add()([inputs, attention_output])  # add resid\n",
        "\n",
        "  # LayerNorm and Feed Forward\n",
        "  X_norm = tf.keras.layers.LayerNormalization(epsilon=1e-6)(X)\n",
        "  ff_output = tf.keras.layers.Conv1D(filters=ff_dim, kernel_size=1, activation=tf.keras.activations.gelu)(X_norm)\n",
        "  ff_output = tf.keras.layers.Dropout(dropout)(ff_output)\n",
        "  ff_output = tf.keras.layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff_output)\n",
        "  X = tf.keras.layers.Add()([X, ff_output])\n",
        "\n",
        "  return X\n",
        "\n",
        "# Final Transformer Model\n",
        "def Transformer_Skeleton(\n",
        "  input_shape,\n",
        "  head_size=128,\n",
        "  num_heads=4,\n",
        "  ff_dim=256,\n",
        "  num_transformer_blocks=6,\n",
        "  mlp_units=[128, 64, 32],\n",
        "  lambda_l2=5e-3,\n",
        "  dropout=0.3,\n",
        "  mlp_dropout=0.2,\n",
        "):\n",
        "  inputs = keras.Input(shape=input_shape)\n",
        "  X = PositionalEncoding()(inputs)\n",
        "\n",
        "  for _ in range(num_transformer_blocks):\n",
        "    X = transformer_encoder(X, head_size, num_heads, ff_dim, dropout)\n",
        "\n",
        "  X = tf.keras.layers.GlobalAveragePooling1D(name='avg_pool_sk')(X)\n",
        "\n",
        "  for idx, units in enumerate(mlp_units):\n",
        "    X = build_dense(units, lambda_l2=lambda_l2, activation='gelu', name=f'dense_sk{idx+1}')(X)\n",
        "    X = tf.keras.layers.Dropout(mlp_dropout)(X)\n",
        "\n",
        "  outputs = tf.keras.layers.Dense(6, activation='softmax', name=f'dense_sk{len(mlp_units)+1}')(X)\n",
        "\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "8W6DOiGVGh-U",
        "outputId": "03e02439-bf6c-43d8-e9ec-c6ba1c14b8a9"
      },
      "outputs": [],
      "source": [
        "sk_trn_exemple = Transformer_Skeleton(input_shape)\n",
        "plot_model(sk_trn_exemple, show_shapes=True, show_layer_names=True, dpi=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XF9KxRQhQir_",
        "outputId": "91bdfc28-984c-4cd8-ca87-3c877b915631"
      },
      "outputs": [],
      "source": [
        "total_params = sk_trn_exemple.count_params()\n",
        "trainable_params = sum([tf.keras.backend.count_params(p) for p in sk_trn_exemple.trainable_weights])\n",
        "non_trainable_params = sum([tf.keras.backend.count_params(p) for p in sk_trn_exemple.non_trainable_weights])\n",
        "\n",
        "print(f\"Total params: {total_params:,} ({total_params * 4 / (1024 ** 2):.2f} MB)\")\n",
        "print(f\"Trainable params: {trainable_params:,} ({trainable_params * 4 / (1024 ** 2):.2f} MB)\")\n",
        "print(f\"Non-trainable params: {non_trainable_params:,} ({non_trainable_params * 4 / (1024 ** 2):.2f} B)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nsh-pl3uG1Kv"
      },
      "source": [
        "#### 2.1.3 - Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A7rLhM5JG6Eu"
      },
      "outputs": [],
      "source": [
        "def build_encoder(enc_input_shape:tuple, code_size:int, layer_type:str='lstm'):\n",
        "\n",
        "  num_units = enc_input_shape[1]\n",
        "\n",
        "  X_input = tf.keras.Input(tuple(enc_input_shape), name='sk_input')\n",
        "\n",
        "  # GRU layers\n",
        "  X = build_AE_layer(num_units, name='enc_rnn_sk0', layer_type=layer_type)(X_input)\n",
        "  X = build_AE_layer(num_units, name='enc_rnn_sk1', layer_type=layer_type)(X)\n",
        "  X = build_AE_layer(num_units, name='enc_rnn_sk2', layer_type=layer_type)(X)\n",
        "  X = build_AE_layer(code_size, name='enc_rnn_sk3', layer_type=layer_type)(X)\n",
        "\n",
        "  encoder_model = tf.keras.Model(inputs=X_input, outputs=X, name=\"Encoder_Skeleton\")\n",
        "\n",
        "  return encoder_model\n",
        "\n",
        "def build_decoder(dec_input_shape:tuple, feature_dim:int, layer_type:str='lstm'):\n",
        "\n",
        "  decoder_input = tf.keras.Input(dec_input_shape, name='decoder_input')\n",
        "\n",
        "  X = build_AE_layer(feature_dim, name='dec_rnn_sk0', layer_type=layer_type)(decoder_input)\n",
        "\n",
        "  decoder_model = tf.keras.Model(inputs=decoder_input, outputs=X, name=\"Decoder_Skeleton\")\n",
        "\n",
        "  return decoder_model\n",
        "\n",
        "def Autoencoder_Skeleton(input_shape:tuple, code_size:int=65, layer_type:str='lstm'):\n",
        "  # define encoder\n",
        "  encoder_model = build_encoder(input_shape, code_size, layer_type)\n",
        "\n",
        "  # get dimensions\n",
        "  input_shape_dec = encoder_model.output_shape[1:]\n",
        "  feature_dim_dec = input_shape[-1]\n",
        "\n",
        "  # define decoder\n",
        "  decoder_model = build_decoder(input_shape_dec, feature_dim_dec, layer_type)\n",
        "\n",
        "  # connect encoder and decoder\n",
        "  inputs = encoder_model.input\n",
        "  encoded_seq = encoder_model(inputs)\n",
        "  outputs = decoder_model(encoded_seq)\n",
        "\n",
        "  autoencoder = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"RNN_Autoencoder\")\n",
        "\n",
        "  return autoencoder, encoder_model, decoder_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "id": "C-RVItguG8QS",
        "outputId": "96525100-f5fe-4516-c95d-96b1c0250f49"
      },
      "outputs": [],
      "source": [
        "sk_AE_model, sk_enc_model, sk_dec_model = Autoencoder_Skeleton(input_shape)\n",
        "sk_enc_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "UDRkWYcLXDRH",
        "outputId": "da77d286-afb5-4293-8964-5c8e778e8250"
      },
      "outputs": [],
      "source": [
        "sk_dec_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 230
        },
        "id": "TJEgShw4h8Ho",
        "outputId": "0c094196-b1b4-478e-b26e-5d5c67135c87"
      },
      "outputs": [],
      "source": [
        "sk_AE_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WY64lelXHIpM"
      },
      "source": [
        "### 2.2 - Speed Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qw9YnpO7HL35"
      },
      "outputs": [],
      "source": [
        "def GRU_Speed(input_shape:tuple, lambda_l2:float=0, bidirectional:bool=False):\n",
        "\n",
        "  X_input = tf.keras.Input(input_shape, name='sp_input')\n",
        "\n",
        "  # dense layer\n",
        "  X = build_dense(units=input_shape[1], activation='relu', name='dense_input_sp')(X_input)\n",
        "\n",
        "  # GRU layers\n",
        "  X =  build_gru(128, return_sequences=True, bidirectional=bidirectional, name=\"gru_sp0\")(X)\n",
        "  X =  build_gru(128, return_sequences=True, bidirectional=bidirectional, name=\"gru_sp1\")(X)\n",
        "  X =  build_gru(64, return_sequences=True, bidirectional=bidirectional, name=\"gru_sp2\")(X)\n",
        "  X =  build_gru(64, return_sequences=False, bidirectional=bidirectional, name=\"gru_sp3\")(X)\n",
        "\n",
        "  # dense and regularization layers\n",
        "  X = build_dense(units=50, lambda_l2=None, activation='relu', name='dense_sp0')(X)\n",
        "  X = tf.keras.layers.Dropout(0.5)(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=-1, name='batchnorm_sp')(X)\n",
        "\n",
        "  # dense layers with decreasing units\n",
        "  for idx, units in enumerate([64, 32, 16]):\n",
        "    X = build_dense(units=units, lambda_l2=lambda_l2, activation='relu', name=f'dense_sp{idx+1}')(X)\n",
        "\n",
        "  # output layer\n",
        "  X = build_dense(units=6, activation='softmax', name=f'dense_sp{idx+2}')(X)\n",
        "\n",
        "  model = tf.keras.Model(inputs=X_input, outputs=X,  name='SpeedModel')\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "kz15L_pDHPwI",
        "outputId": "e5916598-0fff-4395-d932-68cf7335cb41"
      },
      "outputs": [],
      "source": [
        "num_joints = len(_c.joints_to_include)\n",
        "target_size = 50\n",
        "input_shape = (target_size, num_joints)\n",
        "\n",
        "sp_gru_exemple = GRU_Speed(input_shape)\n",
        "\n",
        "sp_gru_exemple.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdQZ25m6HYDk"
      },
      "source": [
        "### 2.3 - Foot Pressure Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chch9Jq4Ha6o"
      },
      "outputs": [],
      "source": [
        "def CNN_Foot(input_shape=(128, 48, 1), lambda_l2:float=0.0):\n",
        "\n",
        "  X_input = tf.keras.Input(input_shape, name='ft_input')\n",
        "\n",
        "  # Conv Block 1\n",
        "  X = build_conv(16, (3, 3), activation='relu', padding='same', name='conv_ft0')(X_input)\n",
        "  X = build_conv(16, (3, 3), activation='relu', padding='same', name='conv_ft1')(X)\n",
        "  X = tf.keras.layers.MaxPooling2D((2, 2), name='pool_ft1')(X)\n",
        "\n",
        "  # Conv Block 2\n",
        "  X = build_conv(32, (3, 3), activation='relu', padding='same', name='conv_ft2')(X)\n",
        "  X = build_conv(32, (3, 3), activation='relu', padding='same', name='conv_ft3')(X)\n",
        "  X = tf.keras.layers.MaxPooling2D((2, 2), name='pool_ft2')(X)\n",
        "\n",
        "  # Conv Block 3\n",
        "  X = build_conv(64, (3, 3), activation='relu', padding='same', name='conv_ft4')(X)\n",
        "  X = build_conv(64, (3, 3), activation='relu', padding='same', name='conv_ft5')(X)\n",
        "  X = tf.keras.layers.MaxPooling2D((2, 2), name='pool_ft3')(X)\n",
        "\n",
        "  # Conv Block 4\n",
        "  X = build_conv(128, (3, 3), activation='relu', padding='same', name='conv_ft6')(X)\n",
        "  X = tf.keras.layers.MaxPooling2D((2, 2), name='pool_ft4')(X)\n",
        "\n",
        "  # Flatten + Projection to feature vector\n",
        "  X = tf.keras.layers.Flatten(name='flatten_ft')(X)\n",
        "  X = build_dense(units=512, activation='relu', name='dense_ft0')(X)\n",
        "  X = build_dense(units=256, activation='relu', name='dense_ft1')(X)\n",
        "  X = build_dense(units=128, activation='relu', name='dense_ft2')(X)\n",
        "  X = tf.keras.layers.Dropout(0.5, name='dropout_ft')(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=-1, name='batchn_ft')(X)\n",
        "\n",
        "  # FC layers\n",
        "  for idx, units in enumerate([128, 64, 32]):\n",
        "    X = build_dense(units=units, lambda_l2=lambda_l2, activation='relu', name=f'dense_ft{idx+3}')(X)\n",
        "\n",
        "  X = build_dense(units=6, activation='softmax', name=f'dense_ft{idx+4}')(X)\n",
        "\n",
        "  # Build the model\n",
        "  model = tf.keras.Model(inputs=X_input, outputs=X, name='FooTModel')\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "n3mJ55I5HfsH",
        "outputId": "7c46a6e2-9972-424b-cdb8-42842833b63f"
      },
      "outputs": [],
      "source": [
        "input_shape = (128, 48, 1)\n",
        "\n",
        "ft_cnn_exemple = CNN_Foot(input_shape)\n",
        "ft_cnn_exemple.summary()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcW_zhcHGpQ-"
      },
      "source": [
        "## 3 - Leave-One-Subject-Out Cross-Validation (LOSO CV)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Rv0UEaUKZHd"
      },
      "source": [
        "\n",
        "We will define two functions, **define_model** that build the model based on datat type and some other parameters, and **LOSO_CV** that performs Leave-One-Subject-Out Cross-Validation for training and evaluating our models on skeleton, foot or speed data. The idea is to **train on all subjects except one**, and **test on the left-out subject**, rotating through all subjects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxtCG9YAvxql"
      },
      "source": [
        "### 3.1 - Single Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtS4yRNuMd7k"
      },
      "outputs": [],
      "source": [
        "def define_model(\n",
        "  data_type:str,\n",
        "  model_type:str,\n",
        "  target_size:int=50,\n",
        "  num_features:int=42,\n",
        "  img_size:tuple=(128,48,1),\n",
        "  lambda_l2:float=0.0,\n",
        "  bidirectional:bool=False,\n",
        "):\n",
        "\n",
        "  if data_type=='skeleton':\n",
        "    SKELETON_INPUT_SHAPE = (target_size, num_features)\n",
        "\n",
        "    if model_type=='gru':\n",
        "      model = GRU_Skeleton(input_shape=SKELETON_INPUT_SHAPE,\n",
        "                           lambda_l2=lambda_l2,\n",
        "                           bidirectional=bidirectional)\n",
        "\n",
        "    else: # model_type=='transformer':\n",
        "      model = Transformer_Skeleton(input_shape=SKELETON_INPUT_SHAPE)\n",
        "\n",
        "  elif data_type=='speed':\n",
        "    SPEED_INPUT_SHAPE = (target_size, num_features)\n",
        "    model = GRU_Speed(input_shape=SPEED_INPUT_SHAPE,\n",
        "                      lambda_l2=lambda_l2,\n",
        "                      bidirectional=bidirectional)\n",
        "\n",
        "  else: # data_type=='foot'\n",
        "    model = CNN_Foot(input_shape=img_size, lambda_l2=lambda_l2)\n",
        "\n",
        "  return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iY_UoemGMhXP"
      },
      "source": [
        "**Step-by-Step guide on how LOSO_CV works**:\n",
        "\n",
        "1. **Subject Loop (LOSO)**\n",
        "   For each subject:\n",
        "\n",
        "   * That subject is used **exclusively for validation**, while all others are used for training.\n",
        "   * Augmented samples (like flipped ones) are **excluded from validation**.\n",
        "\n",
        "2. **Dataset Creation**\n",
        "\n",
        "   * Training and validation datasets are created with proper preprocessing (e.g.: normalization, cropping, joint selection, ecc).\n",
        "\n",
        "4. **Train Classifier**\n",
        "\n",
        "   * Trains the classifier model.\n",
        "   * Evaluates the model on the left-out subject and computes:\n",
        "     * Time informations\n",
        "     * RAM informations\n",
        "     * Confusion Matrix\n",
        "     * Classification Report\n",
        "     * Accuracy\n",
        "\n",
        "5. **Cleanup**\n",
        "\n",
        "   * After each subject, it cleans up memory and cached datasets.\n",
        "   * Clears TensorFlow backend session.\n",
        "\n",
        "6. **Returns Results**\n",
        "\n",
        "    * A dictionary with:\n",
        "\n",
        "      * Training histories for AE and classifier\n",
        "      * Time taken per subject\n",
        "      * RAM usage\n",
        "      * Confusion matrices\n",
        "      * Classification reports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DsmM--twV3DO",
        "outputId": "224550c4-c8f5-4864-fb98-aa5a86524cea"
      },
      "outputs": [],
      "source": [
        "print(tf.config.list_physical_devices('GPU'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Cx3Df9CUHo3"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/70763324/how-to-print-the-maximum-memory-used-during-kerass-model-fit\n",
        "class MemoryLoggingCallback(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.memory_log = {}  # store memory usage per epoch\n",
        "\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    gpu_dict = tf.config.experimental.get_memory_info('GPU:0')\n",
        "    current_gb = float(gpu_dict['current']) / (1024 ** 3)\n",
        "    peak_gb = float(gpu_dict['peak']) / (1024 ** 3)\n",
        "\n",
        "    self.memory_log[epoch] = {'current_gb': current_gb,\n",
        "                              'peak_gb': peak_gb}\n",
        "\n",
        "    # tf.print('\\nGPU memory details [current: {:.2f} GB, peak: {:.2f} GB]'.format(current_gb, peak_gb))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v74mFdYkhVST"
      },
      "outputs": [],
      "source": [
        "def LOSO_CV(\n",
        "  reference_df:pd.DataFrame=None,\n",
        "  test_reference_df:pd.DataFrame=None,\n",
        "  select_x_subj:list=None,\n",
        "  data_type:str='skeleton',\n",
        "  model_type:str='gru',\n",
        "  joints:list=list(range(0,32)),\n",
        "  target_size:int=50,\n",
        "  layer_type:str='lstm',\n",
        "  img_size:tuple=(128,48,1),\n",
        "  clean_data:bool=True,\n",
        "  norm:bool=True,\n",
        "  crop_type:str='aggressive_center',\n",
        "  center_ft:bool=True,\n",
        "  bidirectional:bool=False,\n",
        "  batch_size:int=30,\n",
        "  num_epochs_clf:int=200,\n",
        "  patience_clf:int=30,\n",
        "  lambda_l2:float=1e-2,\n",
        "  lr:float=1e-4,\n",
        "  data_dir:str=None,\n",
        "):\n",
        "\n",
        "  print_params(**locals())\n",
        "  dict_history, dict_times, dict_memory = {}, {}, {}\n",
        "  dict_cm, dict_clf_rep, dict_acc = {}, {}, {}\n",
        "  if test_reference_df is not None:\n",
        "    test_results = {'cm':{}, 'clf_rep':{}, 'acc':{}}\n",
        "\n",
        "  sorted_subjects = sorted(reference_df['subject'].unique(), key=lambda x: int(x.replace('subject', '')))\n",
        "  if isinstance(select_x_subj, list):\n",
        "    sorted_subjects = [sorted_subjects[i-1] for i in select_x_subj]\n",
        "\n",
        "  for subject in sorted_subjects:\n",
        "\n",
        "    print(f'\\n=== {subject.upper()} ===\\n')\n",
        "\n",
        "    # create train and validation reference data\n",
        "    train_reference_df, valid_reference_df = reference_df[reference_df['subject']!=subject], reference_df[reference_df['subject']==subject]\n",
        "    valid_reference_df = valid_reference_df[~valid_reference_df.index.str.contains('flipped')] # no augmented data for the validation set\n",
        "    if test_reference_df is not None:\n",
        "      test_reference_df = test_reference_df[~test_reference_df.index.str.contains('flipped')] # no augmented data for the test set # ADD\n",
        "\n",
        "    if crop_type=='split_subsequence':\n",
        "      chunk_counts = [count_chunks(fp, joints, clean_data=True, norm=True, target_size=target_size) for fp in train_reference_df.index]\n",
        "      train_steps = int(np.ceil(sum(chunk_counts)/batch_size))\n",
        "    else:\n",
        "      train_steps = int(np.ceil(len(train_reference_df)/batch_size))\n",
        "\n",
        "    valid_steps = int(np.ceil(len(valid_reference_df)/batch_size))\n",
        "    if test_reference_df is not None:\n",
        "      test_steps = int(np.ceil(len(test_reference_df)/batch_size))# ADD\n",
        "\n",
        "    # common args to create train and validation sets\n",
        "    common_args = {'data_type':data_type,\n",
        "                   'joints':joints,\n",
        "                   'target_size':target_size,\n",
        "                   'img_size':img_size,\n",
        "                   'clean_data':clean_data,\n",
        "                   'norm':norm,\n",
        "                   'center_ft':center_ft,\n",
        "                   'batch_size':batch_size}\n",
        "\n",
        "    train_data = create_dataset(**common_args,\n",
        "                                reference_df=train_reference_df,\n",
        "                                shuffle=True, crop_type=crop_type,\n",
        "                                cache_file=f'train_{data_type}_{subject}')\n",
        "\n",
        "    crop_type_valid = 'aggressive_center' if crop_type=='split_subsequence' else crop_type\n",
        "    valid_data = create_dataset(**common_args,\n",
        "                                reference_df=valid_reference_df,\n",
        "                                shuffle=False, crop_type=crop_type_valid,\n",
        "                                cache_file=f'valid_{data_type}_{subject}')\n",
        "\n",
        "    if test_reference_df is not None:\n",
        "        test_data = create_dataset(**common_args,\n",
        "                                   reference_df=test_reference_df,\n",
        "                                   shuffle=False, crop_type=crop_type_valid,\n",
        "                                   cache_file=f'test_{data_type}_{subject}')\n",
        "\n",
        "    num_features = len(joints) * 3 if data_type == 'skeleton' else len(joints) if data_type == 'speed' else 0\n",
        "    # initialize classifier\n",
        "    model_clf = define_model(data_type=data_type, model_type=model_type,\n",
        "                             target_size=target_size, num_features=num_features,\n",
        "                             img_size=img_size, lambda_l2=lambda_l2,bidirectional=bidirectional)\n",
        "\n",
        "    model_clf.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    early_stop_callback_clf = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience_clf, restore_best_weights=True)\n",
        "    memory_callback_clf = MemoryLoggingCallback()\n",
        "\n",
        "    start_time_clf = time.time()\n",
        "    history = model_clf.fit(train_data,\n",
        "                            epochs=num_epochs_clf,\n",
        "                            steps_per_epoch=train_steps,\n",
        "                            validation_data=valid_data,\n",
        "                            validation_steps=valid_steps,\n",
        "                            callbacks=[early_stop_callback_clf, memory_callback_clf],\n",
        "                            verbose=0)\n",
        "\n",
        "    delta_time_clf = time.time() - start_time_clf\n",
        "\n",
        "    # show results\n",
        "    print_time_info(subject, history, delta_time_clf)\n",
        "    plot_history(history, subject)\n",
        "    cm, df_report, acc = compute_metrics(model_clf, valid_reference_df, valid_data, valid_steps, subject)\n",
        "    if test_reference_df is not None:\n",
        "      cm_test, df_report_test, acc_test = compute_metrics(model_clf, test_reference_df, test_data, test_steps, 'subject12')\n",
        "    print('\\n')\n",
        "\n",
        "    dict_times[subject], dict_history[subject], dict_memory[subject] = delta_time_clf, history, memory_callback_clf.memory_log\n",
        "    dict_cm[subject], dict_clf_rep[subject], dict_acc[subject] = cm, df_report, acc\n",
        "    if test_reference_df is not None:\n",
        "      test_results['cm'][subject], test_results['clf_rep'][subject], test_results['acc'][subject] = cm_test, df_report_test, acc_test\n",
        "\n",
        "    # remove cache and clear session\n",
        "    del train_data, valid_data, model_clf, history, early_stop_callback_clf, memory_callback_clf\n",
        "    remove_cache_from_drive(cache_data=f'train_{data_type}_{subject}', data_dir=data_dir)\n",
        "    remove_cache_from_drive(cache_data=f'valid_{data_type}_{subject}', data_dir=data_dir)\n",
        "    if test_reference_df is not None:\n",
        "      remove_cache_from_drive(cache_data=f'test_{data_type}_{subject}', data_dir=data_dir)\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "  # ensemble reuslts\n",
        "  results = {\n",
        "      'dict_memory':dict_memory,\n",
        "      'dict_history':dict_history,\n",
        "      'dict_times':dict_times,\n",
        "      'dict_cm':dict_cm,\n",
        "      'dict_clf_rep':dict_clf_rep,\n",
        "      'dict_acc':dict_acc\n",
        "      }\n",
        "  if test_reference_df is not None:\n",
        "    results['test_results'] = test_results\n",
        "\n",
        "  return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FGL6nGXMvvRO"
      },
      "source": [
        "### 3.2 - AE + Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VN9D6SeRxMg"
      },
      "source": [
        "Basically the idea is to train an autoencoder using all joints (features) and let it extract the relevant informations, then pass the extracted features through the classifier.\n",
        "\n",
        "We define a new LOSO CV function and a new function to choose the model (between classifier and AE)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ua6U1-IXdtx"
      },
      "outputs": [],
      "source": [
        "def define_model_AE(\n",
        "  model_type:str,\n",
        "  layer_type:str=None,\n",
        "  num_features:int=42,\n",
        "  code_size:int=None,\n",
        "):\n",
        "\n",
        "  if model_type=='AE':\n",
        "    SKELETON_INPUT_SHAPE = (50, num_features) # length of the sequence + #joints\n",
        "    AE, Enc, Dec = Autoencoder_Skeleton(input_shape=SKELETON_INPUT_SHAPE,\n",
        "                                        code_size=code_size,\n",
        "                                        layer_type=layer_type)\n",
        "    return AE, Enc, Dec\n",
        "\n",
        "  elif model_type=='clf':\n",
        "    SKELETON_INPUT_SHAPE = (50, code_size) # length of the sequence + #extracted features\n",
        "\n",
        "    model = GRU_Skeleton(input_shape=SKELETON_INPUT_SHAPE,\n",
        "                         lambda_l2=5e-3,\n",
        "                         bidirectional=False)\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GixOF7PvRwah"
      },
      "outputs": [],
      "source": [
        "def LOSO_CV_AE(\n",
        "  reference_df:pd.DataFrame=None,\n",
        "  select_x_subj:list=None,\n",
        "  code_size:int=50,\n",
        "  layer_type:str='lstm',\n",
        "  batch_size:int=30,\n",
        "  num_epochs_clf:int=200,\n",
        "  patience_clf:int=30,\n",
        "  lr:float=1e-4,\n",
        "  num_epochs_AE:int=5000,\n",
        "  patience_AE:int=10,\n",
        "  data_dir:str=None,\n",
        "):\n",
        "\n",
        "  assert layer_type in ['lstm', 'gru'], f\"layer_type must be either 'lstm' or 'gru' but is set to: {layer_type}\"\n",
        "\n",
        "  print_params_AE(**locals())\n",
        "\n",
        "  dict_history_clf, dict_times_clf, dict_memory_clf = {}, {}, {}\n",
        "  dict_cm_clf, dict_clf_rep_clf, dict_acc_clf = {}, {}, {}\n",
        "  dict_history_AE, dict_times_AE, dict_memory_AE = {}, {}, {}\n",
        "\n",
        "  # AE only for skeleton data\n",
        "  data_type = 'skeleton'\n",
        "\n",
        "  sorted_subjects = sorted(reference_df['subject'].unique(), key=lambda x: int(x.replace('subject', '')))\n",
        "  if isinstance(select_x_subj, list):\n",
        "    sorted_subjects = [sorted_subjects[i-1] for i in select_x_subj]\n",
        "\n",
        "  for subject in sorted_subjects:\n",
        "\n",
        "    print(f'\\n=== {subject.upper()} ===\\n')\n",
        "\n",
        "    # create train and validation reference data\n",
        "    train_reference_df, valid_reference_df = reference_df[reference_df['subject']!=subject], reference_df[reference_df['subject']==subject]\n",
        "    valid_reference_df = valid_reference_df[~valid_reference_df.index.str.contains('flipped')] # no augmented data for the validation set\n",
        "\n",
        "    train_steps = int(np.ceil(len(train_reference_df)/batch_size))\n",
        "    valid_steps = int(np.ceil(len(valid_reference_df)/batch_size))\n",
        "\n",
        "    # 1) We need to create the dataset to train AE\n",
        "    train_data_AE = create_dataset_AE(reference_df=train_reference_df, train_or_valid='train',\n",
        "                                      train_AE=True, batch_size=batch_size, shuffle=True,\n",
        "                                      cache_file=f'train_AE_{data_type}_{subject}')\n",
        "\n",
        "    valid_data_AE = create_dataset_AE(reference_df=valid_reference_df, train_or_valid='valid',\n",
        "                                      train_AE=True, batch_size=batch_size, shuffle=False,\n",
        "                                      cache_file=f'valid_AE_{data_type}_{subject}')\n",
        "\n",
        "    # 2) initialize and train AE\n",
        "    num_features = 3*len(list(range(0,32)))\n",
        "    model_AE, model_encoder, model_decoder = define_model_AE(model_type='AE', layer_type=layer_type, num_features=num_features, code_size=code_size)\n",
        "\n",
        "    model_AE.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                     loss=tf.keras.losses.MeanSquaredError(),\n",
        "                     metrics=['mse'])\n",
        "\n",
        "    early_stop_callback_AE = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience_clf, restore_best_weights=True)\n",
        "    memory_callback_AE = MemoryLoggingCallback()\n",
        "\n",
        "    start_time_AE = time.time()\n",
        "    history_AE = model_AE.fit(train_data_AE,\n",
        "                              epochs=num_epochs_AE,\n",
        "                              steps_per_epoch=train_steps,\n",
        "                              validation_data=valid_data_AE,\n",
        "                              validation_steps=valid_steps,\n",
        "                              callbacks=[early_stop_callback_AE, memory_callback_AE],\n",
        "                              verbose=0)\n",
        "\n",
        "    delta_time_AE = time.time() - start_time_AE\n",
        "\n",
        "    # 3) show AE results\n",
        "    print_time_info(subject, history_AE, delta_time_AE)\n",
        "    plot_history_AE(history_AE, subject)\n",
        "    print('\\n')\n",
        "\n",
        "    # 4) create dataset for the classifier (extract features using AE)\n",
        "    train_data_clf = create_dataset_AE(reference_df=train_reference_df, train_or_valid='train',\n",
        "                                       train_AE=False, encoder_model=model_encoder,\n",
        "                                       batch_size=batch_size, shuffle=True,\n",
        "                                       cache_file=f'train_clf_{data_type}_{subject}')\n",
        "\n",
        "    valid_data_clf = create_dataset_AE(reference_df=valid_reference_df, train_or_valid='valid',\n",
        "                                       train_AE=False, encoder_model=model_encoder,\n",
        "                                       batch_size=batch_size, shuffle=False,\n",
        "                                       cache_file=f'valid_clf_{data_type}_{subject}')\n",
        "\n",
        "    # 5) initialize and train classifier\n",
        "    model_clf = define_model_AE(model_type='clf', code_size=code_size)\n",
        "\n",
        "    model_clf.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=lr),\n",
        "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    early_stop_callback_clf = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience_clf, restore_best_weights=True)\n",
        "    memory_callback_clf = MemoryLoggingCallback()\n",
        "\n",
        "    start_time_clf = time.time()\n",
        "    history_clf = model_clf.fit(train_data_clf,\n",
        "                                epochs=num_epochs_clf,\n",
        "                                steps_per_epoch=train_steps,\n",
        "                                validation_data=valid_data_clf,\n",
        "                                validation_steps=valid_steps,\n",
        "                                callbacks=[early_stop_callback_clf, memory_callback_clf],\n",
        "                                verbose=0)\n",
        "\n",
        "    delta_time_clf = time.time() - start_time_clf\n",
        "\n",
        "    # 6) show classifier results\n",
        "    print_time_info(subject, history_clf, delta_time_clf)\n",
        "    plot_history(history_clf, subject)\n",
        "    cm, df_report, acc = compute_metrics(model_clf, valid_reference_df, valid_data_clf, valid_steps, subject)\n",
        "    print('\\n')\n",
        "\n",
        "    # save all results\n",
        "    dict_times_clf[subject], dict_history_clf[subject], dict_memory_clf[subject] = delta_time_clf, history_clf, memory_callback_clf.memory_log\n",
        "    dict_cm_clf[subject], dict_clf_rep_clf[subject], dict_acc_clf[subject] = cm, df_report, acc\n",
        "    dict_history_AE[subject], dict_times_AE[subject], dict_memory_AE[subject] = history_AE, delta_time_AE, memory_callback_AE.memory_log\n",
        "\n",
        "    # remove cache and clear session\n",
        "    del train_data_clf, valid_data_clf, train_data_AE, valid_data_AE\n",
        "    del model_AE, model_encoder, model_decoder, early_stop_callback_AE, memory_callback_AE\n",
        "    del model_clf, history_clf, early_stop_callback_clf, memory_callback_clf\n",
        "\n",
        "    remove_cache_from_drive(cache_data=f'train_AE_{data_type}_{subject}', data_dir=data_dir)\n",
        "    remove_cache_from_drive(cache_data=f'valid_AE_{data_type}_{subject}', data_dir=data_dir)\n",
        "    remove_cache_from_drive(cache_data=f'train_clf_{data_type}_{subject}', data_dir=data_dir)\n",
        "    remove_cache_from_drive(cache_data=f'valid_clf_{data_type}_{subject}', data_dir=data_dir)\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "  # ensemble results\n",
        "  results = {\n",
        "      'dict_memory':dict_memory_clf,\n",
        "      'dict_history':dict_history_clf,\n",
        "      'dict_times':dict_times_clf,\n",
        "      'dict_cm':dict_cm_clf,\n",
        "      'dict_clf_rep':dict_clf_rep_clf,\n",
        "      'dict_acc':dict_acc_clf,\n",
        "      'dict_history_AE':dict_history_AE,\n",
        "      'dict_times_AE':dict_times_AE,\n",
        "      'dict_memory_AE':dict_memory_AE\n",
        "      }\n",
        "\n",
        "  return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H4pJOjK9ImA"
      },
      "source": [
        "## 4 - Skeleton LOSO CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67SQ-PGMKIE9"
      },
      "source": [
        "### 4.1 - GRU Classifier\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpA0-yXsIRIj"
      },
      "source": [
        "| Crop Type           | #Frames | Avg. Accuracy |\n",
        "|-----------------------|---------|----------------|\n",
        "| top                   | 100     | 0.4515         |\n",
        "| bottom                | 100     | 0.8712         |\n",
        "| center                | 100     | 0.8129         |\n",
        "| random                | 100     | 0.7538         |\n",
        "| 40perc                | 100     | 0.8879         |\n",
        "| aggressive_center     | 50      | 0.8621         |\n",
        "| aggressive_random     | 50      | 0.8780         |\n",
        "| split_subsequence     | 50      | 0.8924         |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-pZ_tI8rYgZH",
        "outputId": "6ea04d6f-62cd-4ac3-bb30-e501d9204940"
      },
      "outputs": [],
      "source": [
        "our_params = {'reference_df': loso_reference_df,\n",
        "              'model_type': 'gru',\n",
        "              'data_type': 'skeleton',\n",
        "              'target_size': 50,\n",
        "              'joints': _c.joints_to_include,\n",
        "              'clean_data': True,\n",
        "              'norm': True,\n",
        "              'crop_type': 'split_subsequence',\n",
        "              'batch_size': 30,\n",
        "              'bidirectional': False,\n",
        "              'lambda_l2': 5e-3,\n",
        "              'lr': 1e-4,\n",
        "              'num_epochs_clf': 200,\n",
        "              'patience_clf': 30,\n",
        "              'data_dir': data_dir,\n",
        "              }\n",
        "\n",
        "results = LOSO_CV(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzc5t8InYgZI",
        "outputId": "1cf73637-791d-4392-d26d-3e13b6af3dbb"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "mReEc37XYgZJ",
        "outputId": "448340ac-fce0-4c9b-92b0-a7435d7df462"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "l5uUhUnkYgZJ",
        "outputId": "2f13bac2-064b-4158-fdde-f388ae1cec91"
      },
      "outputs": [],
      "source": [
        "print_overall_epochs(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "ze2LsAFWYgZJ",
        "outputId": "f52b66ba-6efd-4105-c9e8-88078c29c3fe"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "txrmB6ikSrTf",
        "outputId": "4ec1a148-4e6a-475c-ec19-410ac2998914"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lZZ7xdSYgZJ",
        "outputId": "77644070-1eec-463c-e652-e43fc2ef080f"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "sHIXsWQqYgZJ",
        "outputId": "f95a59fe-5e6e-44c3-eab6-0973ce25ba20"
      },
      "outputs": [],
      "source": [
        "print_overall_accuracies(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXhWRfF1YgZK",
        "outputId": "b2b3bc74-42a4-457b-92e9-d6e580d88fe5"
      },
      "outputs": [],
      "source": [
        "print_overall_max_gpu_memory_usage(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vw_v5J0oAgW"
      },
      "source": [
        "### 4.2 - Bidirectional GRU Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5biBFH8oAgX"
      },
      "source": [
        "The best crop strategy was selected and the same architecture as the previous section but using bidirectional GRU is used to perform LOSO-CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "nEqCUh23oAgY",
        "outputId": "61fcf15a-7ad6-4f8b-e054-e23358b2b603"
      },
      "outputs": [],
      "source": [
        "our_params = {'reference_df': loso_reference_df,\n",
        "              'model_type': 'gru',\n",
        "              'data_type': 'skeleton',\n",
        "              'target_size': 50,\n",
        "              'joints': _c.joints_to_include,\n",
        "              'clean_data': True,\n",
        "              'norm': True,\n",
        "              'crop_type': 'split_subsequence',\n",
        "              'batch_size': 30,\n",
        "              'bidirectional': True,\n",
        "              'lambda_l2': 5e-3,\n",
        "              'lr': 1e-4,\n",
        "              'num_epochs_clf': 200,\n",
        "              'patience_clf': 30,\n",
        "              'data_dir': data_dir,\n",
        "              }\n",
        "\n",
        "results = LOSO_CV(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep2Zn60SoAgZ",
        "outputId": "4eb06305-e9a9-4d00-fd86-e72bc1c5a593"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "J3O5NwZJoAgZ",
        "outputId": "6188df93-f603-4f75-815b-074a3bdbef51"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "sEJkAVGPoAgZ",
        "outputId": "09d12431-0fc6-4e34-dad4-db7e289f9e44"
      },
      "outputs": [],
      "source": [
        "print_overall_epochs(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "8X1UUtF8oAga",
        "outputId": "12133c8b-5317-4a47-c32b-cb8a8a28f32f"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KsF4apSS9Xs",
        "outputId": "a6aeffb4-5e1a-4b0e-849a-ea6571dac792"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "IkyeFhv0oAga",
        "outputId": "55cecf24-201f-4383-9006-e8a0e831b557"
      },
      "outputs": [],
      "source": [
        "print_overall_accuracies(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5j5XGZPboAga",
        "outputId": "c67ef726-adff-4868-b9ab-66a4e6b937a9"
      },
      "outputs": [],
      "source": [
        "print_overall_max_gpu_memory_usage(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JjyGLxiSGWtv"
      },
      "source": [
        "### 4.3 - Transformer Classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qjNJQXDh1XGs",
        "outputId": "e3c3667a-a888-4b57-e835-c92cd6495a6d"
      },
      "outputs": [],
      "source": [
        "our_params = {'reference_df': loso_reference_df,\n",
        "              'model_type': 'transformer',\n",
        "              'data_type': 'skeleton',\n",
        "              'target_size': 50,\n",
        "              'joints': _c.joints_to_include,\n",
        "              'clean_data': True,\n",
        "              'norm': True,\n",
        "              'crop_type': 'split_subsequence',\n",
        "              'batch_size': 30,\n",
        "              'lambda_l2': 5e-3,\n",
        "              'lr': 1e-4,\n",
        "              'num_epochs_clf': 200,\n",
        "              'patience_clf': 30,\n",
        "              'data_dir':data_dir,\n",
        "              }\n",
        "\n",
        "results = LOSO_CV(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IvvucawrKtqq",
        "outputId": "57d9ee8a-7e3e-48b0-deaa-7d4c184a2d7e"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "A39R0dRdiDsl",
        "outputId": "2c53c9ad-bfd2-4db9-f3a0-d0f47812cd07"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "LA8PUGaqKtqq",
        "outputId": "8e26a6bf-ccbb-498d-da29-0e61e5cee86f"
      },
      "outputs": [],
      "source": [
        "print_overall_epochs(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "DAQL8Z_zKtqr",
        "outputId": "e92643e6-b1d2-48c3-d328-27b92b4d4292"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN6EpJtgVpe6",
        "outputId": "969e2583-1692-473a-e8d6-b6039f309e15"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "O8mhad-XSjTR",
        "outputId": "be492a76-4d53-42cf-ee72-b4ea23d85307"
      },
      "outputs": [],
      "source": [
        "print_overall_accuracies(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4ssgasT774w",
        "outputId": "e898c18c-aa1f-4e99-dcc4-1d722bc6faa6"
      },
      "outputs": [],
      "source": [
        "print_overall_max_gpu_memory_usage(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MT6GRX8ORoMF"
      },
      "source": [
        "### 4.4 - Autoencoder + Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD_QDayMKeOZ"
      },
      "source": [
        "| Code Size | Avg Accuracy |\n",
        "|-----------|--------------|\n",
        "| 50 | 0.8515 |\n",
        "| 60 | 0.8818 |\n",
        "| 70 | 0.8481 |\n",
        "| 80 | 0.8368 |\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KHeKIrlGWK_3",
        "outputId": "99ea6a89-8d72-4db4-8ca7-2ea4c21b6d3a"
      },
      "outputs": [],
      "source": [
        "our_params = {'reference_df': loso_reference_df,\n",
        "              'select_x_subj': [1,2,3,4,5,6],\n",
        "              'layer_type': 'lstm',\n",
        "              'code_size': 60,\n",
        "              'batch_size': 30,\n",
        "              'num_epochs_clf': 200,\n",
        "              'patience_clf': 30,\n",
        "              'num_epochs_AE': 5000,\n",
        "              'patience_AE': 10,\n",
        "              'lr': 1e-4,\n",
        "              'data_dir': data_dir,\n",
        "              }\n",
        "\n",
        "results1 = LOSO_CV_AE(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7QBFevl9rO-"
      },
      "outputs": [],
      "source": [
        "# to save results\n",
        "# with open('SK_LOSO_LSTM_AE50a.pkl', 'wb') as f:\n",
        "#   pickle.dump(results1, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z1KF4LyCmlf9",
        "outputId": "ac1a508f-b714-471d-cfef-e33aa28673c9"
      },
      "outputs": [],
      "source": [
        "our_params = {'reference_df': loso_reference_df,\n",
        "              'select_x_subj': [7,8,9,10,11],\n",
        "              'layer_type': 'lstm',\n",
        "              'code_size': 60,\n",
        "              'batch_size': 30,\n",
        "              'num_epochs_clf': 200,\n",
        "              'patience_clf': 30,\n",
        "              'num_epochs_AE': 5000,\n",
        "              'patience_AE': 10,\n",
        "              'lr': 1e-4,\n",
        "              'data_dir': data_dir,\n",
        "              }\n",
        "\n",
        "results2 = LOSO_CV_AE(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dGWCx64vmlf9"
      },
      "outputs": [],
      "source": [
        "# # to save results\n",
        "# with open('SK_LOSO_LSTM_AE50b.pkl', 'wb') as f:\n",
        "#   pickle.dump(results2, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZfILvM4mlf9"
      },
      "outputs": [],
      "source": [
        "# to load results\n",
        "# with open('SK_LOSO_LSTM_AE50a.pkl', 'rb') as f:\n",
        "#   results1 = pickle.load(f)\n",
        "\n",
        "# with open('SK_LOSO_LSTM_AE50b.pkl', 'rb') as f:\n",
        "#   results2 = pickle.load(f)\n",
        "\n",
        "results = merge_results(results1, results2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1T6Ls-pLF7a",
        "outputId": "6b10d259-e363-4fb5-fceb-f748c124ea62"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results, clf_or_ae='AE')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TabGxDrjLF7c",
        "outputId": "b8a39472-5cae-494f-d105-f73fc81f0701"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "aosnkwL4LF7d",
        "outputId": "5ae27ec6-4570-4d77-b8b8-d455e782363d"
      },
      "outputs": [],
      "source": [
        "print_overall_history_AE(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "CTtor3sOLF7d",
        "outputId": "71bb944d-d0f3-43ed-fddc-2016cdcac98c"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "nAbHzWnCZENP",
        "outputId": "2043143f-0324-403a-dddb-1fecaa6c6d8e"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIhOWbqrZvzh",
        "outputId": "83429bd4-93f1-4836-eb5c-a4437d42ba5a"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc7uFkOkKegi"
      },
      "source": [
        "## 5 - Speed LOSO CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jdyQVpqgKegi",
        "outputId": "f2f6d7b4-c317-4122-85e4-dd48ab697241"
      },
      "outputs": [],
      "source": [
        "our_params = {'reference_df': loso_reference_df,\n",
        "              'model_type': 'gru',\n",
        "              'data_type': 'speed',\n",
        "              'target_size': 50,\n",
        "              'joints': _c.joints_to_include,\n",
        "              'clean_data': True,\n",
        "              'norm': True,\n",
        "              'crop_type': 'split_subsequence',\n",
        "              'batch_size': 30,\n",
        "              'lambda_l2': 5e-3,\n",
        "              'lr': 1e-4,\n",
        "              'num_epochs_clf': 200,\n",
        "              'patience_clf': 30,\n",
        "              'data_dir': data_dir,\n",
        "              }\n",
        "\n",
        "results = LOSO_CV(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4vzPv8nKegj",
        "outputId": "46f02a1f-97a0-4524-e1ec-edb781e070c4"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "cWb0hnNtKegj",
        "outputId": "7241af43-8071-4fca-8273-1f412ca42136"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "KXMRvsGuKegk",
        "outputId": "f8f34067-53ee-49fb-acb5-dac0aa54d099"
      },
      "outputs": [],
      "source": [
        "print_overall_epochs(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "GmpBis-iKegk",
        "outputId": "3a9561f9-113e-402f-f49b-5de59ac278c9"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC8JdEsYZ8nI",
        "outputId": "9a19f356-b2db-4cfc-cec9-13251ab3d514"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "7aHom2OXKegk",
        "outputId": "0cc36480-d98c-48cb-b0bf-a8ccfde54fa3"
      },
      "outputs": [],
      "source": [
        "print_overall_accuracies(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0UYJh4lKegk",
        "outputId": "c35e5cf1-33d2-4394-a596-1c96a7723bab"
      },
      "outputs": [],
      "source": [
        "print_overall_max_gpu_memory_usage(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3J6h8TQ98ll"
      },
      "source": [
        "## 6 - Foot Pressure LOSO CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Y1mYlZRL98ln",
        "outputId": "ae8013ef-e8d3-48bc-84e8-9acad5336478"
      },
      "outputs": [],
      "source": [
        "our_params = {'reference_df': loso_reference_df,\n",
        "              'data_type': 'foot',\n",
        "              'img_size': (128,48,1),\n",
        "              'norm': True,\n",
        "              'center_ft': True,\n",
        "              'batch_size': 30,\n",
        "              'lambda_l2': 5e-3,\n",
        "              'lr': 5e-5,\n",
        "              'num_epochs_clf': 200,\n",
        "              'patience_clf': 30,\n",
        "              'data_dir':data_dir,\n",
        "              }\n",
        "\n",
        "results = LOSO_CV(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DyjH94m98lo",
        "outputId": "682356b5-bcaf-4d6e-e641-165e10f573a6"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "KI8jFLmv98lo",
        "outputId": "4a1a2ea4-d122-431c-c214-65a8428856a5"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "_TaXfTIY98lo",
        "outputId": "03c2fbad-af51-42cb-ae51-177a87154502"
      },
      "outputs": [],
      "source": [
        "print_overall_epochs(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "nJ5wbYU998lo",
        "outputId": "b79c0370-45e1-4a1d-c6b4-eff32f5b8adf"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaEv3H8PbkwZ",
        "outputId": "dacaf5ce-eab8-4874-d20f-99d774b6d791"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "Mklofx7x98lp",
        "outputId": "907ecf80-31f2-4892-f7e7-49e539fb2176"
      },
      "outputs": [],
      "source": [
        "print_overall_accuracies(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CCvrc7_98lp",
        "outputId": "210ee6c7-fdc3-4048-c098-55d0bf4ad3f7"
      },
      "outputs": [],
      "source": [
        "print_overall_max_gpu_memory_usage(results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8pDKe_WKWp1"
      },
      "source": [
        "## 7 - Fusion Model LOSO CV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG8iPqlzKipV"
      },
      "source": [
        "In this section we build the fusion model that takes as input skeleton, foot and speed data. The three data types are passed through the best classifier among the one of the previous sections, the classifiers are then cut before the batch normalization layer such that the features extracted from each one are concatenated and passed through a final MLP.\n",
        "\n",
        "4 configurations of fusion models were tried:\n",
        "1. All data types (Skeleton + Foot + Speed)\n",
        "2. Skeleton + Foot\n",
        "3. Skeleton + Speed\n",
        "4. Speed + Foot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxrmMBCER6fo"
      },
      "source": [
        "### 7.1 - Define Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0h5Es6zsKe_Z"
      },
      "outputs": [],
      "source": [
        "def build_fusion_model(model_dict, data_types_to_include, freeze_base_models:bool=True, lambda_l2_fus:float=5e-3):\n",
        "  inputs = []\n",
        "  outputs = []\n",
        "\n",
        "  if 'skeleton' in data_types_to_include:\n",
        "    sk_model = model_dict['skeleton']\n",
        "    sk_model.trainable = not freeze_base_models\n",
        "    sk_trunc = tf.keras.Model(inputs=sk_model.input,\n",
        "                              outputs=sk_model.get_layer(f'dense_sk0').output)\n",
        "                              #outputs=sk_model.get_layer(f'avg_pool_sk').output)\n",
        "\n",
        "    inputs.append(sk_trunc.input)\n",
        "    outputs.append(sk_trunc.output)\n",
        "\n",
        "  if 'speed' in data_types_to_include:\n",
        "    sp_model = model_dict['speed']\n",
        "    sp_model.trainable = not freeze_base_models\n",
        "    sp_trunc = tf.keras.Model(inputs=sp_model.input,\n",
        "                              outputs=sp_model.get_layer(f'dense_sp0').output)\n",
        "    inputs.append(sp_trunc.input)\n",
        "    outputs.append(sp_trunc.output)\n",
        "\n",
        "  if 'foot' in data_types_to_include:\n",
        "    ft_model = model_dict['foot']\n",
        "    ft_model.trainable = not freeze_base_models\n",
        "    ft_trunc = tf.keras.Model(inputs=ft_model.input,\n",
        "                              outputs=ft_model.get_layer(f'dense_ft2').output)\n",
        "    inputs.append(ft_trunc.input)\n",
        "    outputs.append(ft_trunc.output)\n",
        "\n",
        "  # concatenate selected features\n",
        "  X = tf.keras.layers.Concatenate(name='concat_features')(outputs)\n",
        "\n",
        "  # FC layers\n",
        "  X = tf.keras.layers.Dropout(0.3, name='dropout_fus')(X)\n",
        "  X = tf.keras.layers.BatchNormalization(axis=-1, name='batchnorm_fus')(X)\n",
        "\n",
        "  for idx, units in enumerate([128, 64, 32]):\n",
        "    X = build_dense(units=units, lambda_l2=lambda_l2_fus, activation='relu', name=f'dense_fus{idx}')(X)\n",
        "  X = build_dense(units=6, activation='softmax', name=f'dense_fus{idx+1}')(X)\n",
        "\n",
        "  fusion_model = tf.keras.Model(inputs=inputs, outputs=X, name='FusionModel')\n",
        "  return fusion_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYISo668PEPK",
        "outputId": "b18e9dfd-5c20-4ffe-8fb8-2af716e7085c"
      },
      "outputs": [],
      "source": [
        "model_dict = {'skeleton': sk_gru_exemple,\n",
        "              'speed': sp_gru_exemple,\n",
        "              'foot': ft_cnn_exemple}\n",
        "\n",
        "data_types_to_include = ['skeleton', 'speed', 'foot']\n",
        "fus_model_exemple = build_fusion_model(model_dict, data_types_to_include)\n",
        "\n",
        "plot_model(fus_model_exemple, show_shapes=True, show_layer_names=True, dpi=100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yF9z8Eh7XJq3",
        "outputId": "b9748a0c-148b-4ec1-b4d7-bdf83dd46975"
      },
      "outputs": [],
      "source": [
        "total_params = fus_model_exemple.count_params()\n",
        "trainable_params = sum([tf.keras.backend.count_params(p) for p in fus_model_exemple.trainable_weights])\n",
        "non_trainable_params = sum([tf.keras.backend.count_params(p) for p in fus_model_exemple.non_trainable_weights])\n",
        "\n",
        "print(f\"Total params: {total_params:,} ({total_params * 4 / (1024 ** 2):.2f} MB)\")\n",
        "print(f\"Trainable params: {trainable_params:,} ({trainable_params * 4 / (1024 ** 2):.2f} MB)\")\n",
        "print(f\"Non-trainable params: {non_trainable_params:,} ({non_trainable_params * 4 / (1024 ** 2):.2f} B)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGTEAjjsR9sX"
      },
      "source": [
        "### 7.2 - Define LOSO CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "by2kpV5rWN8N"
      },
      "outputs": [],
      "source": [
        "def LOSO_CV_fus(\n",
        "  reference_df:pd.DataFrame=None,\n",
        "  select_x_subj:list=None,\n",
        "  data_types_to_include:list=None,\n",
        "  freeze_base_models:bool=True,\n",
        "  batch_size:int=30,\n",
        "  num_epochs_fus:int=200,\n",
        "  patience_fus:int=30,\n",
        "  lambda_l2_fus:float=1e-3,\n",
        "  lr_fus:float=1e-4,\n",
        "  data_dir:str=None,\n",
        "):\n",
        "  # initilaize objects to store LOSO CV infos\n",
        "  results = {}\n",
        "  for data_type in data_types_to_include + ['fusion']:\n",
        "    results[data_type] = {'dict_history':{}, 'dict_times':{}, 'dict_memory':{},\n",
        "                          'dict_cm':{}, 'dict_clf_rep':{}, 'dict_acc':{}}\n",
        "\n",
        "  sorted_subjects = sorted(reference_df['subject'].unique(), key=lambda x: int(x.replace('subject', '')))\n",
        "  if isinstance(select_x_subj, list):\n",
        "    sorted_subjects = [sorted_subjects[i-1] for i in select_x_subj]\n",
        "\n",
        "  for subject in sorted_subjects:\n",
        "\n",
        "    print(f'\\n=== {subject.upper()} ===\\n')\n",
        "\n",
        "    # object to store models\n",
        "    model_dict = {}\n",
        "\n",
        "    # create train and validation reference data\n",
        "    train_reference_df, valid_reference_df = reference_df[reference_df['subject']!=subject], reference_df[reference_df['subject']==subject]\n",
        "    valid_reference_df = valid_reference_df[~valid_reference_df.index.str.contains('flipped')] # no augmented data for the validation set\n",
        "\n",
        "    joints = [0,1,2,3,4,11,18,19,20,21,22,23,24,25]\n",
        "\n",
        "    for data_type in data_types_to_include:\n",
        "\n",
        "      if data_type == 'foot':\n",
        "        train_steps = int(np.ceil(len(train_reference_df)/batch_size))\n",
        "      else:\n",
        "        chunk_counts = [count_chunks(fp, joints, clean_data=True, norm=True, target_size=50) for fp in train_reference_df.index]\n",
        "        train_steps = int(np.ceil(sum(chunk_counts)/batch_size))\n",
        "      valid_steps = int(np.ceil(len(valid_reference_df)/batch_size))\n",
        "\n",
        "      # common args to create train and validation sets\n",
        "      common_args = {'data_type':data_type,\n",
        "                    'joints':joints,\n",
        "                    'target_size':50,\n",
        "                    'img_size':(128,48,1),\n",
        "                    'clean_data':True,\n",
        "                    'norm':True,\n",
        "                    'center_ft':True,\n",
        "                    'batch_size':batch_size}\n",
        "\n",
        "      train_data = create_dataset(**common_args, reference_df=train_reference_df, crop_type='split_subsequence',\n",
        "                                  shuffle=True, cache_file=f'train_{data_type}_{subject}')\n",
        "\n",
        "      valid_data = create_dataset(**common_args, reference_df=valid_reference_df, crop_type='aggressive_center',\n",
        "                                  shuffle=False, cache_file=f'valid_{data_type}_{subject}')\n",
        "\n",
        "      # initialize classifier\n",
        "      num_features = len(joints)*3 if data_type=='skeleton' else len(joints) if data_type=='speed' else 0\n",
        "      model_type = 'gru' if data_type == 'skeleton' else ('gru' if data_type == 'speed' else 'cnn')\n",
        "      # model_type = 'transformer' if data_type == 'skeleton' else ('gru' if data_type == 'speed' else 'cnn')\n",
        "\n",
        "      lr_clf = 5e-5 if data_type=='foot' else 1e-4\n",
        "\n",
        "      model_clf = define_model(data_type=data_type, model_type=model_type,\n",
        "                               target_size=50, num_features=num_features,\n",
        "                               img_size=(128,48,1), lambda_l2=5e-3,bidirectional=False)\n",
        "\n",
        "      model_clf.compile(optimizer= tf.keras.optimizers.Adam(learning_rate=lr_clf),\n",
        "                        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                        metrics=['accuracy'])\n",
        "\n",
        "      early_stop_callback_clf = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True)\n",
        "      memory_callback_clf = MemoryLoggingCallback()\n",
        "\n",
        "      start_time_clf = time.time()\n",
        "      history_clf = model_clf.fit(train_data, epochs=200, steps_per_epoch=train_steps,\n",
        "                                  validation_data=valid_data, validation_steps=valid_steps,\n",
        "                                  callbacks=[early_stop_callback_clf, memory_callback_clf], verbose=0)\n",
        "\n",
        "      delta_time_clf = time.time() - start_time_clf\n",
        "\n",
        "      # save results\n",
        "      cm, df_report, acc = compute_metrics_fus(model_clf, valid_reference_df, valid_data, valid_steps)\n",
        "\n",
        "      results[data_type]['dict_times'][subject], results[data_type]['dict_history'][subject], results[data_type]['dict_memory'][subject] = delta_time_clf, history_clf, memory_callback_clf.memory_log\n",
        "      results[data_type]['dict_cm'][subject], results[data_type]['dict_clf_rep'][subject], results[data_type]['dict_acc'][subject] = cm, df_report, acc\n",
        "\n",
        "      # save model to create the fusion one\n",
        "      model_dict[data_type] = model_clf\n",
        "\n",
        "      del train_data, valid_data, model_clf\n",
        "\n",
        "    ################\n",
        "    # FUSION MODEL #\n",
        "    ################\n",
        "    chunk_counts = [count_chunks(fp, joints, clean_data=True, norm=True, target_size=50) for fp in train_reference_df.index]\n",
        "    train_steps = int(np.ceil(sum(chunk_counts)/batch_size))\n",
        "\n",
        "    # create dataset\n",
        "    train_data_fus = create_fusion_dataset(reference_df=train_reference_df,\n",
        "                                           data_types_to_include=data_types_to_include,\n",
        "                                           shuffle=True, cache_file=f'train_fus_{subject}')\n",
        "\n",
        "    valid_data_fus = create_fusion_dataset(reference_df=valid_reference_df, train_or_valid='valid',\n",
        "                                           data_types_to_include=data_types_to_include,\n",
        "                                           shuffle=False, cache_file=f'valid_fus_{subject}')\n",
        "\n",
        "    # initialize and train model\n",
        "    fus_model = build_fusion_model(model_dict, data_types_to_include, freeze_base_models, lambda_l2_fus)\n",
        "    fus_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr_fus),\n",
        "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "    early_stop_callback_fus = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=patience_fus, restore_best_weights=True)\n",
        "    memory_callback_fus = MemoryLoggingCallback()\n",
        "\n",
        "    start_time_fus = time.time()\n",
        "    history_fus = fus_model.fit(train_data_fus, epochs=num_epochs_fus, steps_per_epoch=train_steps,\n",
        "                                validation_data=valid_data_fus, validation_steps=valid_steps,\n",
        "                                callbacks=[early_stop_callback_fus, memory_callback_fus], verbose=0)\n",
        "\n",
        "    delta_time_fus = time.time() - start_time_fus\n",
        "\n",
        "    # show and save results\n",
        "    cm, df_report, acc = compute_metrics_fus(fus_model, valid_reference_df, valid_data_fus, valid_steps)\n",
        "\n",
        "    results['fusion']['dict_times'][subject], results['fusion']['dict_history'][subject], results['fusion']['dict_memory'][subject] = delta_time_fus, history_fus, memory_callback_fus.memory_log\n",
        "    results['fusion']['dict_cm'][subject], results['fusion']['dict_clf_rep'][subject], results['fusion']['dict_acc'][subject] = cm, df_report, acc\n",
        "\n",
        "    print_time_info_fus(subject=subject, results=results, data_types_to_include=data_types_to_include)\n",
        "    plot_history_fus(subject=subject, results=results, data_types_to_include=data_types_to_include)\n",
        "    plot_metrics_fus(subject=subject, results=results, data_types_to_include=data_types_to_include)\n",
        "\n",
        "    # remove cache and clear session\n",
        "    for data_type in data_types_to_include + ['fus']:\n",
        "      remove_cache_from_drive(cache_data=f'train_{data_type}_{subject}', data_dir=data_dir)\n",
        "      remove_cache_from_drive(cache_data=f'valid_{data_type}_{subject}', data_dir=data_dir)\n",
        "\n",
        "    tf.keras.backend.clear_session()\n",
        "    gc.collect()\n",
        "\n",
        "  return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCVrslj1Tt6a"
      },
      "source": [
        "### 7.3 LOSO CV Fusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbmy-Gxi6mbr"
      },
      "source": [
        "#### 7.3.1 - Skeleton + Foot Pressure Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pvBuYU8f6mbs",
        "outputId": "319cf601-ee6e-4591-e89e-1c498ced04bd"
      },
      "outputs": [],
      "source": [
        "our_params = {\n",
        "    'reference_df':loso_reference_df,\n",
        "    'data_types_to_include':['skeleton', 'foot'],\n",
        "    'freeze_base_models': False,\n",
        "    'batch_size':30,\n",
        "    'num_epochs_fus':200,\n",
        "    'patience_fus':10,\n",
        "    'lambda_l2_fus':5e-3,\n",
        "    'lr_fus':5e-5,\n",
        "    'data_dir':data_dir\n",
        "}\n",
        "\n",
        "results = LOSO_CV_fus(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mGS5Imir6mbt",
        "outputId": "fd7790f6-3244-4bb1-d01d-baf036f30ac7"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "QaSjNcmz6mbt",
        "outputId": "fd11decf-5028-4bd4-dbcd-0b329aca4919"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "UtjWELtT6mbt",
        "outputId": "cd6f81a4-a6e1-47e4-9c63-eda9f10a0127"
      },
      "outputs": [],
      "source": [
        "print_overall_epochs(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "ZG8A3lyH6mbu",
        "outputId": "518d57ac-f882-4425-f309-520c4bf1aa00"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEuZLvubb8S4",
        "outputId": "202fa80d-a003-4ab6-93ce-ef3ab4c61878"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "RrMhpY3i6mbu",
        "outputId": "2ddc167b-a936-4cf2-dc35-923ed1d74423"
      },
      "outputs": [],
      "source": [
        "show_multiple_accuracies(results, ['fusion', 'skeleton', 'foot'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cs_BGVxI6mbu",
        "outputId": "cc4a22ea-262d-4a07-ed8f-2e02d9709056"
      },
      "outputs": [],
      "source": [
        "print_overall_max_gpu_memory_usage(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oujypvgi8HN0"
      },
      "source": [
        "#### 7.3.2 - Skeleton + Speed Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cVpQYk49Fg4E",
        "outputId": "373d9d77-9b0c-45e0-9f37-ac7fc152f88b"
      },
      "outputs": [],
      "source": [
        "our_params = {\n",
        "    'reference_df':loso_reference_df,\n",
        "    'data_types_to_include':['skeleton', 'speed'],\n",
        "    'freeze_base_models': False,\n",
        "    'batch_size':30,\n",
        "    'num_epochs_fus':200,\n",
        "    'patience_fus':10,\n",
        "    'lambda_l2_fus':5e-3,\n",
        "    'lr_fus':5e-5,\n",
        "    'data_dir':data_dir\n",
        "}\n",
        "\n",
        "results = LOSO_CV_fus(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m9SDkHB8HN1",
        "outputId": "646afdd0-e796-4cd8-cbd4-8aefccab45a0"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H_nksdZ8HN1",
        "outputId": "5aab7311-7507-4167-d173-ad107038925b"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sUSq6mSN8HN1",
        "outputId": "02d51d08-b6ed-492e-d26f-c3e71b4ad95a"
      },
      "outputs": [],
      "source": [
        "print_overall_epochs(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CRSG_oag8HN1",
        "outputId": "238302dd-203e-4401-d175-1d5c844e789a"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP2jqMYDcHQJ",
        "outputId": "4fcdfdce-974e-4ed3-d943-9999b164f14e"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wCh24f48HN2",
        "outputId": "fde189bc-7d31-4e8b-ba5c-fd25f4527cd4"
      },
      "outputs": [],
      "source": [
        "show_multiple_accuracies(results, ['fusion', 'skeleton', 'speed'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "us4IUSsL8HN2",
        "outputId": "91b7b3dd-eaf7-459b-9e2a-343ce32cc4db"
      },
      "outputs": [],
      "source": [
        "print_overall_max_gpu_memory_usage(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOedbGyE8Sh_"
      },
      "source": [
        "#### 7.3.3 - Speed + Foot Pressure Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "C-_6OQvG8Sh_",
        "outputId": "cf9d86cf-4b58-43e4-c292-0cf824ac0dff"
      },
      "outputs": [],
      "source": [
        "our_params = {\n",
        "    'reference_df':loso_reference_df,\n",
        "    'data_types_to_include':['speed', 'foot'],\n",
        "    'freeze_base_models': False,\n",
        "    'batch_size':30,\n",
        "    'num_epochs_fus':200,\n",
        "    'patience_fus':10,\n",
        "    'lambda_l2_fus':5e-3,\n",
        "    'lr_fus':5e-5,\n",
        "    'data_dir':data_dir\n",
        "}\n",
        "\n",
        "results = LOSO_CV_fus(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "655gnRZy8SiA",
        "outputId": "f6a6e3df-e684-4888-ce00-de96ce901e9a"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "wyQv0HQb8SiA",
        "outputId": "8bea0e3c-da37-4b65-e039-ec10139b89bf"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "OEn9IIEr8SiA",
        "outputId": "9b302499-d19e-40e2-e8a3-41d2831c84e8"
      },
      "outputs": [],
      "source": [
        "print_overall_epochs(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "chh1SdiD8SiA",
        "outputId": "4d7f36a7-808b-4e9d-ee6e-4b8754c5f15a"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmaJ803ScY_J",
        "outputId": "bfa25b8e-0c8f-4894-b322-c3566df77c38"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "A3pSFYCC8SiB",
        "outputId": "970da28e-c877-4572-c7bc-6dc97d3936cc"
      },
      "outputs": [],
      "source": [
        "show_multiple_accuracies(results, ['fusion', 'speed', 'foot'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxNTG7Z08SiB",
        "outputId": "607a3d9b-b3b8-403e-d6b5-492238f2c596"
      },
      "outputs": [],
      "source": [
        "print_overall_max_gpu_memory_usage(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siI9wHvy8icI"
      },
      "source": [
        "#### 7.3.4 - Skeleton + Speed + Foot Pressure Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6gA57EIiE2Vn",
        "outputId": "64ab821e-0a12-4e38-9005-58cbe74b968e"
      },
      "outputs": [],
      "source": [
        "our_params = {\n",
        "    'reference_df':loso_reference_df,\n",
        "    'select_x_subj': [1,2,3,4,5,6],\n",
        "    'data_types_to_include':['skeleton', 'speed', 'foot'],\n",
        "    'freeze_base_models': False,\n",
        "    'batch_size':30,\n",
        "    'num_epochs_fus':200,\n",
        "    'patience_fus':10,\n",
        "    'lambda_l2_fus':5e-3,\n",
        "    'lr_fus':5e-5,\n",
        "    'data_dir':data_dir\n",
        "}\n",
        "\n",
        "results1 = LOSO_CV_fus(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_z4ZWRxWSwQS"
      },
      "outputs": [],
      "source": [
        "# to save results\n",
        "with open('FUSION_SK_SP_FT_LOSOa.pkl', 'wb') as f:\n",
        "  pickle.dump(results1, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PWvbWWiFJRkR",
        "outputId": "fb2015e2-e0d0-4612-c009-c65bdd8f700b"
      },
      "outputs": [],
      "source": [
        "our_params = {\n",
        "    'reference_df':loso_reference_df,\n",
        "    'select_x_subj': [7,8,9,10,11],\n",
        "    'data_types_to_include':['skeleton', 'speed', 'foot'],\n",
        "    'freeze_base_models': False,\n",
        "    'batch_size':30,\n",
        "    'num_epochs_fus':200,\n",
        "    'patience_fus':10,\n",
        "    'lambda_l2_fus':5e-3,\n",
        "    'lr_fus':5e-5,\n",
        "    'data_dir':data_dir\n",
        "}\n",
        "\n",
        "results2 = LOSO_CV_fus(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB_rD1BDJRkS"
      },
      "outputs": [],
      "source": [
        "# to save results\n",
        "with open('FUSION_SK_SP_FT_LOSOb.pkl', 'wb') as f:\n",
        "  pickle.dump(results2, f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cTgLznLABmxr"
      },
      "outputs": [],
      "source": [
        "# to load results\n",
        "with open('FUSION_SK_SP_FT_LOSOa.pkl', 'rb') as f:\n",
        "  results1 = pickle.load(f)\n",
        "\n",
        "with open('FUSION_SK_SP_FT_LOSOb.pkl', 'rb') as f:\n",
        "  results2 = pickle.load(f)\n",
        "\n",
        "results = merge_loso_results(results1, results2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j6xDcsVdCt7T",
        "outputId": "cc0e4c80-1642-47b0-c053-9af3ae360183"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "vmusUezBCt7U",
        "outputId": "3c9034bb-8a50-4621-8061-042e94bdac4f"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "zzc8txncCt7V",
        "outputId": "22e616bc-47f9-4a61-c89a-e6fd8c7999cd"
      },
      "outputs": [],
      "source": [
        "print_overall_epochs(results, 'clf', 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "FW5GuvFrCt7X",
        "outputId": "b059a547-c643-4a6e-8e06-22201eb0e2f8"
      },
      "outputs": [],
      "source": [
        "print_overall_cm(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5V1uY0oCt7Y",
        "outputId": "8a6efba7-ad84-4761-ab9d-f7e33eb16f72"
      },
      "outputs": [],
      "source": [
        "print_overall_clf_report(results, 'fusion')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        },
        "id": "JE5OXQTGCt7Y",
        "outputId": "13636288-a9a2-4f39-fb1c-e5db54ffd09a"
      },
      "outputs": [],
      "source": [
        "show_multiple_accuracies(results, ['fusion', 'skeleton', 'speed', 'foot'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-p__9XtgcxW"
      },
      "source": [
        "## 8 - Final Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G65b531CkHjv"
      },
      "source": [
        "### 8.1 - Transformer Classifier LOSO CV with Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LyouB8zeKvkh",
        "outputId": "02ed866d-9d33-4620-edeb-8f06bf822738"
      },
      "outputs": [],
      "source": [
        "our_params = {'reference_df': loso_reference_df,\n",
        "              'test_reference_df': test_reference_df,\n",
        "              'model_type': 'transformer',\n",
        "              'data_type': 'skeleton',\n",
        "              'target_size': 50,\n",
        "              'joints': _c.joints_to_include,\n",
        "              'clean_data': True,\n",
        "              'norm': True,\n",
        "              'crop_type': 'split_subsequence',\n",
        "              'batch_size': 30,\n",
        "              'lambda_l2': 5e-3,\n",
        "              'lr': 1e-4,\n",
        "              'num_epochs_clf': 200,\n",
        "              'patience_clf': 30,\n",
        "              'data_dir':data_dir,\n",
        "              }\n",
        "\n",
        "results = LOSO_CV(**our_params)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TFbBrSRWK9bH",
        "outputId": "2541e872-d4e1-460e-e591-9ba770cfc1dd"
      },
      "outputs": [],
      "source": [
        "print_overall_time(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "yCq-0BqQK9bH",
        "outputId": "a30ce783-1dca-4a14-e3a5-3279127459e5"
      },
      "outputs": [],
      "source": [
        "print_overall_history(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1oMLOsHK9bJ"
      },
      "outputs": [],
      "source": [
        "test_results = results['test_results']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AExmh-KxK9bJ",
        "outputId": "d310a070-769c-4da7-c65a-bd2ce5b72068"
      },
      "outputs": [],
      "source": [
        "test_accuracies = []\n",
        "test_precisions = []\n",
        "test_recalls = []\n",
        "\n",
        "for subj, report_df in test_results['clf_rep'].items():\n",
        "    if isinstance(report_df, pd.DataFrame):\n",
        "        precision = report_df.loc['macro avg', 'precision']\n",
        "        recall = report_df.loc['macro avg', 'recall']\n",
        "    else:\n",
        "        precision = report_df['macro avg']['precision']\n",
        "        recall = report_df['macro avg']['recall']\n",
        "\n",
        "    accuracy = test_results['acc'][subj]\n",
        "\n",
        "    test_accuracies.append(accuracy)\n",
        "    test_precisions.append(precision)\n",
        "    test_recalls.append(recall)\n",
        "\n",
        "avg_test_accuracy = np.mean(test_accuracies)\n",
        "avg_test_precision = np.mean(test_precisions)\n",
        "avg_test_recall = np.mean(test_recalls)\n",
        "\n",
        "print(f\"\\n==== AVERAGE TEST METRICS ====\")\n",
        "print(f\"Accuracy: {avg_test_accuracy:.4f}\")\n",
        "print(f\"Precision: {avg_test_precision:.4f}\")\n",
        "print(f\"Recall: {avg_test_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "6SeYbQ12Vyv1",
        "outputId": "03e5d05e-aa11-4a69-cf72-3e172a513bb9"
      },
      "outputs": [],
      "source": [
        "cm_dict = results['test_results']['cm']\n",
        "conf_matrices = list(cm_dict.values())\n",
        "total_cm = np.sum(conf_matrices, axis=0)\n",
        "class_names = ['Normal', 'Antalgic', 'Lurching', 'Steppage', 'Stiff-legged', 'Trendelenburg']\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(total_cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"Test Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mPYoSzEJKEZl"
      },
      "source": [
        "### 8.2 - Final Training with All Exemples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IChr7g_giyF"
      },
      "source": [
        "The final model will be trained on the same subjects used for LOSO CV, meaning all 11 subjects are used for training while the 12th remain as test set.\n",
        "The model weights are then saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeQHK7WmIumj",
        "outputId": "698c4366-9b1f-4bce-ab0f-a2a993e256dd"
      },
      "outputs": [],
      "source": [
        "joints_to_include=_c.joints_to_include\n",
        "target_size=50\n",
        "\n",
        "# Create Training Dataset\n",
        "chunk_counts = [count_chunks(fp, joints_to_include, clean_data=True, norm=True, target_size=target_size) for fp in loso_reference_df.index]\n",
        "train_steps = int(np.ceil(sum(chunk_counts)/30))\n",
        "\n",
        "train_data = create_dataset(reference_df=loso_reference_df, data_type='skeleton', joints=joints_to_include,\n",
        "                            target_size=target_size, clean_data=True, norm=True, crop_type='split_subsequence',\n",
        "                            batch_size=30, shuffle=True, cache_file='final_train_data')\n",
        "\n",
        "# Initialize the Model\n",
        "model = Transformer_Skeleton(input_shape=(target_size, 3*len(joints_to_include)))\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',patience=30,restore_best_weights=True)\n",
        "\n",
        "# Train the Model\n",
        "history = model.fit(train_data, epochs=200, steps_per_epoch=train_steps, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "# Save the Model\n",
        "model.save('transformer_skeleton_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "3IJQXQ-hWPq1",
        "outputId": "02f562dd-7a1e-451c-9eae-519b516826de"
      },
      "outputs": [],
      "source": [
        "def plot_final_hist(history):\n",
        "  fig, axs = plt.subplots(1, 2, figsize=(14, 6)) # do not use predefine fig size (in chapter 0)\n",
        "  # loss\n",
        "  axs[0].plot(history.history['loss'], label='Train loss')\n",
        "  axs[0].set_xlabel('Epoch')\n",
        "  axs[0].set_ylabel('Loss')\n",
        "  axs[0].set_title(f'Loss vs Epoch')\n",
        "  # accuracy\n",
        "  axs[1].plot(history.history['accuracy'], label='Train accuracy')\n",
        "  axs[1].set_xlabel('Epoch')\n",
        "  axs[1].set_ylabel('Accuracy')\n",
        "  axs[1].set_title(f'Accuracy vs Epoch')\n",
        "\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "plot_final_hist(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfLZxssXSxUr"
      },
      "outputs": [],
      "source": [
        "# Train Data\n",
        "train_data_eval = create_dataset(reference_df=loso_reference_df, data_type='skeleton', joints=joints_to_include,\n",
        "                                 target_size=target_size, clean_data=True, norm=True, crop_type='aggressive_center',\n",
        "                                 batch_size=30, shuffle=False, cache_file='final_train_data_eval')\n",
        "train_steps_eval = int(np.ceil(len(loso_reference_df)/30))\n",
        "\n",
        "cm_train, clf_report_train, acc_train = compute_metrics(model, loso_reference_df, train_data_eval, train_steps_eval, verbose=False)\n",
        "\n",
        "# Test Data\n",
        "test_reference_df = test_reference_df[~test_reference_df.index.str.contains('flipped')]\n",
        "test_data_eval = create_dataset(reference_df=test_reference_df, data_type='skeleton', joints=joints_to_include,\n",
        "                                target_size=target_size, clean_data=True, norm=True, crop_type='aggressive_center',\n",
        "                                batch_size=30, shuffle=False, cache_file='final_test_data_eval')\n",
        "test_steps_eval = int(np.ceil(len(test_reference_df)/30))\n",
        "\n",
        "cm_test, clf_report_test, acc_test = compute_metrics(model, test_reference_df, test_data_eval, test_steps_eval, verbose=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KNbjk9ecRjvQ",
        "outputId": "8998ddaf-1876-48a7-fed4-921adf0d6652"
      },
      "outputs": [],
      "source": [
        "print(\"=== TRAIN SET ===\\n\")\n",
        "print(f\"Accuracy: {acc_train:.4f}\\n\")\n",
        "print(\"Classification Report:\\n\", clf_report_train.to_string(), \"\\n\")\n",
        "\n",
        "print(\"\\n\\n=== TEST SET ===\\n\")\n",
        "print(f\"Accuracy: {acc_test:.4f}\\n\")\n",
        "print(\"Classification Report:\\n\", clf_report_test.to_string(), \"\\n\")\n",
        "\n",
        "classes=list(_c.convrt_gait_dict.keys())\n",
        "fig, axs = plt.subplots(1, 2, figsize=(14, 6)) # do not use predefine fig size (in chapter 0)\n",
        "\n",
        "sns.heatmap(cm_train, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes, ax=axs[0])\n",
        "axs[0].set_title('Train CM')\n",
        "axs[0].set_xlabel('Predicted Label')\n",
        "axs[0].set_ylabel('True Label')\n",
        "\n",
        "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes, ax=axs[1])\n",
        "axs[1].set_title('Test CM')\n",
        "axs[1].set_xlabel('Predicted Label')\n",
        "axs[1].set_ylabel('True Label')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGrPvo7_WETr"
      },
      "outputs": [],
      "source": [
        "remove_cache_from_drive(cache_data='final_train_data', data_dir=data_dir)\n",
        "remove_cache_from_drive(cache_data='final_train_data_eval', data_dir=data_dir)\n",
        "remove_cache_from_drive(cache_data='final_test_data_eval', data_dir=data_dir)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nphHJrVqfu6k",
        "RC8sIB8vFEL_",
        "b9eQ8-AhGVSV",
        "GD1IvmPBGZ58",
        "nsh-pl3uG1Kv",
        "XdQZ25m6HYDk",
        "FGL6nGXMvvRO",
        "0H4pJOjK9ImA",
        "67SQ-PGMKIE9",
        "ZzwJx-JTL_Nn",
        "yLlvpNzlq2_g",
        "tTRHCAvD3-mf",
        "Fc_HQ5QP4W_X",
        "aP9s8kGn4dNw",
        "uiuZQmmN4deh",
        "dTpa4vDnYgZF",
        "-vw_v5J0oAgW",
        "JjyGLxiSGWtv",
        "MT6GRX8ORoMF",
        "Rc7uFkOkKegi",
        "C3J6h8TQ98ll",
        "tOedbGyE8Sh_"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
